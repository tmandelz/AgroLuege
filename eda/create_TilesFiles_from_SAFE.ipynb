{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "117f5d6c-dd4c-4c7e-a87c-0a003cbb70fe",
   "metadata": {},
   "source": [
    "# Create bernCrop Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a41e2-811f-4041-a491-86537f052f98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load required modules\n",
    "import cv2\n",
    "import eodal\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from pathlib import Path\n",
    "from eodal.core.sensors import Sentinel2\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from eodal.config import get_settings\n",
    "\n",
    "# make plots larger by default\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "\n",
    "print('eodal version: {}'.format(eodal.__version__))\n",
    "\n",
    "# we need to tell EOdal that we work using a local data source\n",
    "settings = get_settings()\n",
    "settings.USE_STAC = False\n",
    "\n",
    "\n",
    "output_shapefile_path_BERN = f\"../raw_data/LANDKULT/data/BERN_big_bbox.shp\"\n",
    "shapefile_path_landkult = '../raw_data/LANDKULT/data/LANDKULT_NUTZFL.shp'\n",
    "output_shapefile_path_landkult = '../raw_data/LANDKULT/data/LANDKULT_NUTZFL_bern_bbox.shp'\n",
    "output_shapefile_path_landkult_short = '../raw_data/LANDKULT/data/LANDKULT_NUTZFL_short_bern_bbox.shp'\n",
    "output_shapefile_path_landkult_short_eodal = 'D:/Temp/AgroLuege/raw_data/LANDKULT/data/LANDKULT_NUTZFL_short_bern_bbox.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the coordinates\n",
    "x1, y1, x2, y2 = 361630.678100406, 416830.678100406, 5140066.039024595, 5238466.039024595\n",
    "\n",
    "# Create a GeoDataFrame with a single Point geometry\n",
    "geometry = Polygon([(x1, y1), (x2, y1), (x2, y2), (x1, y2)])\n",
    "gdf = gpd.GeoDataFrame(geometry=[geometry], crs=\"EPSG:32632\")\n",
    "\n",
    "# Save the GeoDataFrame to a shapefile\n",
    "gdf.to_file(output_shapefile_path_BERN)\n",
    "\n",
    "# Read the shapefiles into GeoPandas DataFrames\n",
    "gdf1 = gpd.read_file(shapefile_path_landkult).to_crs('EPSG:32632')\n",
    "gdf2 = gpd.read_file(output_shapefile_path_BERN).to_crs('EPSG:32632')\n",
    "\n",
    "# Perform the intersection\n",
    "gdf1_only_beitrag = gdf1[gdf1['BEITRAG'] == 1]\n",
    "intersection_gdf_onlybeitrag = gpd.overlay(gdf1_only_beitrag, gdf2, how='intersection')\n",
    "\n",
    "intersection_gdf_onlybeitrag.to_file(output_shapefile_path_landkult, driver='ESRI Shapefile')\n",
    "intersection_gdf_onlybeitrag[0:100].to_file(output_shapefile_path_landkult_short, driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [100,150,200,300,500,1000]:\n",
    "    intersection_gdf_onlybeitrag.sort_values('Shape_Area',ascending=False)[0:i].plot()\n",
    "    plt.show()\n",
    "    intersection_gdf_onlybeitrag.sort_values('Shape_Area',ascending=False)['Shape_Area'][0:i].hist()\n",
    "    plt.show()\n",
    "\n",
    "for i in [100,150,200,300,500,1000]:\n",
    "    # intersection_gdf.sort_values('Shape_Area',ascending=True)[0:i].plot()\n",
    "    # plt.show()\n",
    "    intersection_gdf_onlybeitrag.sort_values('Shape_Area',ascending=True)['Shape_Area'][0:i].hist()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del intersection_gdf_onlybeitrag,gdf,gdf1,gdf1_only_beitrag,gdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tile_folder_path(data_dir):\n",
    "    paths = []\n",
    "    def get_subdirectories(path, depth=0, max_depth=0):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "\n",
    "        subdirectories = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "        for subdir in subdirectories:\n",
    "            subdir_path = os.path.join(path, subdir)\n",
    "            if subdir_path.endswith('.SAFE'):\n",
    "                paths.append(subdir_path)\n",
    "            get_subdirectories(subdir_path, depth + 1, max_depth)\n",
    "    get_subdirectories(data_dir)\n",
    "    return paths\n",
    "\n",
    "def read_tile_data_from_safe(tile_paths,tile,band_selection = ['B02', 'B03', 'B04', 'B08']):\n",
    "    base_dir = Path(os.path.dirname(os.path.realpath(\"__file__\"))).parent.parent\n",
    "    in_file_aoi = base_dir.joinpath(output_shapefile_path_landkult_short_eodal)\n",
    "\n",
    "    # for path_SAFE in tile_paths[0:5]:\n",
    "    for path_SAFE in tile_paths:\n",
    "        print(path_SAFE)\n",
    "        # read data from .SAFE dataset for the selected AOI and spectral bands\n",
    "        handler = Sentinel2.from_safe(\n",
    "            in_dir=Path(path_SAFE),\n",
    "            vector_features=in_file_aoi,\n",
    "            band_selection=band_selection,\n",
    "            apply_scaling=False # if True scales the reflectance values between 0 and 1\n",
    "\n",
    "        )\n",
    "        # ignore the value if its blackfilled\n",
    "        if handler.is_blackfilled == True:\n",
    "            print(f\"Skip is blackfilled: {path_SAFE}\")\n",
    "            continue\n",
    "        # first resample the spectral bands using bicubic interpolation\n",
    "        handler.resample(\n",
    "            target_resolution=10,\n",
    "            interpolation_method=cv2.INTER_NEAREST_EXACT,\n",
    "            inplace=True\n",
    "        )\n",
    "\n",
    "        # create a numpy array and remove last band\n",
    "        timestamp_tile_data = [handler.to_xarray().to_numpy()[0:4]]\n",
    "        # save tile data\n",
    "        save_tile_data(timestamp_tile_data, tile)\n",
    "\n",
    "def save_tile_data(temp_results_tensor, tile,dataset_data_name=\"data\"):\n",
    "    file_name_tile = f'../raw_data/BernCrop/tiles/{tile}.hdf5'\n",
    "    tile= np.array(temp_results_tensor)\n",
    "    data_shape = tile.shape\n",
    "\n",
    "    with h5py.File(file_name_tile, 'a') as hf:\n",
    "        # Check if the dataset already exists\n",
    "        if dataset_data_name in hf:\n",
    "            dataset = hf[dataset_data_name]\n",
    "        else:\n",
    "            dtype = \"float32\"  # Use the appropriate data type for your data\n",
    "            dataset = hf.create_dataset(dataset_data_name, shape=(0,) + data_shape[1:], dtype=dtype, maxshape=(None,) + data_shape[1:])\n",
    "            \n",
    "        current_size = dataset.shape[0]\n",
    "        new_size = current_size + tile.shape[0]\n",
    "        # Resize the dataset to accommodate the new batch\n",
    "        dataset.resize(new_size, axis=0)\n",
    "        # Append the new batch to the dataset\n",
    "        dataset[current_size:new_size, :] = tile\n",
    "\n",
    "\n",
    "def read_tile_data(tile):\n",
    "    filename_tile = f'../raw_data/BernCrop/tiles/{tile}.hdf5'\n",
    "    # Open the HDF5 file in read mode\n",
    "    with h5py.File(filename_tile, \"r\") as file:\n",
    "        # Check if the \"data\" dataset exists in the file\n",
    "        if \"data\" in file:\n",
    "            # Access the dataset and read its contents into a NumPy array\n",
    "            dataset = file[\"data\"][:]\n",
    "        else:\n",
    "            print(\"Dataset 'data' not found in the HDF5 file.\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tile data and save to dhf5 files\n",
    "tiles = ['T32TLS','T32TLT','T32TMS','T32TMT']\n",
    "for tile in tiles[0:1]:\n",
    "# for tile in tiles:\n",
    "    data_dir = Path('E:/S2_Data_CH22/' + tile)\n",
    "    tile_paths = get_tile_folder_path(data_dir)\n",
    "    read_tile_data_from_safe(tile_paths,tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile1 = read_tile_data(tiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Assuming your original array has shape (3, 4, 3888, 4626)\n",
    "# original_array = np.random.random((3, 4, 3888, 4626))\n",
    "\n",
    "# # Choose the value of x\n",
    "# x =np.floor((3888 * 4626) / (24*24)).astype(int)  # Replace with your desired value\n",
    "# new_shape = (x, 3, 4, 24, 24)\n",
    "\n",
    "# resized_array = original_array.reshape(new_shape)\n",
    "# print(resized_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO: labels\n",
    "# dataset_label_name = \"gt\"\n",
    "# #TODO: define label_tensor\n",
    "# label_shape = result_tensor[:,-1,:,:,-1].shape\n",
    "# label_tensor = result_tensor[:,-1,:,:,-1]\n",
    "\n",
    "# with h5py.File(file_name_bern, 'a') as hf:    \n",
    "#     # Check if the dataset already exists\n",
    "#     if dataset_label_name in hf:\n",
    "#         dataset = hf[dataset_label_name]\n",
    "#     else:\n",
    "#         dtype = \"float32\"  # Use the appropriate data type for your data\n",
    "#         dataset = hf.create_dataset(dataset_label_name, shape=(0,) + label_shape[1:], dtype=dtype, maxshape=(None,) + label_shape[1:])\n",
    "        \n",
    "#     current_size = dataset.shape[0]\n",
    "#     new_size = current_size + label_tensor.shape[0]\n",
    "#     # Resize the dataset to accommodate the new batch\n",
    "#     dataset.resize(new_size, axis=0)\n",
    "#     # Append the new batch to the dataset\n",
    "#     dataset[current_size:new_size, :] = label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the HDF5 file in read mode\n",
    "with h5py.File(file_name_bern, \"r\") as file:\n",
    "    # Check if the \"data\" dataset exists in the file\n",
    "    if \"data\" in file:\n",
    "        # Access the dataset and read its contents into a NumPy array\n",
    "        dataset_b = file[\"data\"][:]\n",
    "    else:\n",
    "        print(\"Dataset 'data' not found in the HDF5 file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
